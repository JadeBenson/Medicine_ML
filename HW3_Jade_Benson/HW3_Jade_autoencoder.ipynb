{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc195d3-d9b7-4012-ad1a-f1233311dde1",
   "metadata": {},
   "source": [
    "**Part A, autoecoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21e7258c-c44a-45b4-af06-a186ba6f185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231146e7-bac1-4803-a9d7-f522b0e998f0",
   "metadata": {},
   "source": [
    "We will use an autoencoder to reduce the dimensionality of the gene data and then return back to the original size. This can help us perform similar classification and prediction tasks as before. I decided to run this on the binary normal vs. tumor all gene dataset since this had fewer labels so I could better practice using autoencoders on a simpler case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "944bc513-44cf-4508-b3ac-b90590a9df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_pickle(\"./all_nt.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48499880-aa9f-4cc2-ad22-826177725916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I realize now that this splitting is perhaps unnecessary for the tasks we perform \n",
    "#we're not predicting onto the test set at the end \n",
    "#we might find a use for the test set in the future though\n",
    "\n",
    "#split 80-20 into training and testing sets \n",
    "train_all, test_all = train_test_split(all_df, test_size = 0.2)\n",
    "\n",
    "train_all_x = train_all.drop(columns = [\"Type\"])\n",
    "#scale the x's \n",
    "train_max = max(np.max(train_all_x))\n",
    "train_all_x = train_all_x/train_max\n",
    "\n",
    "test_all_x = test_all.drop(columns = [\"Type\"])\n",
    "#scale the x's \n",
    "test_max = max(np.max(test_all_x))\n",
    "test_all_x = test_all_x/test_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0385c984-c8b7-4d0c-b0ec-690285a7544b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120, 60483)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0427040b-afd0-4d07-8517-32f7b9e93cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 4s 642ms/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.6904 - val_loss: 0.6889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.6881 - val_loss: 0.6866\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.6858 - val_loss: 0.6842\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.6833 - val_loss: 0.6814\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.6805 - val_loss: 0.6782\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.6773 - val_loss: 0.6742\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.6734 - val_loss: 0.6692\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.6686 - val_loss: 0.6627\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.6627 - val_loss: 0.6544\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.6553 - val_loss: 0.6438\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.6462 - val_loss: 0.6307\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.6353 - val_loss: 0.6148\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.6221 - val_loss: 0.5958\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 489ms/step - loss: 0.6069 - val_loss: 0.5737\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 479ms/step - loss: 0.5893 - val_loss: 0.5487\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 477ms/step - loss: 0.5696 - val_loss: 0.5209\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 487ms/step - loss: 0.5477 - val_loss: 0.4907\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.5240 - val_loss: 0.4587\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.4987 - val_loss: 0.4254\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.4721 - val_loss: 0.3916\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 2s 459ms/step - loss: 0.4447 - val_loss: 0.3577\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 2s 459ms/step - loss: 0.4168 - val_loss: 0.3245\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 2s 484ms/step - loss: 0.3889 - val_loss: 0.2924\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 2s 486ms/step - loss: 0.3613 - val_loss: 0.2621\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.3344 - val_loss: 0.2337\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.3086 - val_loss: 0.2075\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.2840 - val_loss: 0.1837\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.2608 - val_loss: 0.1622\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 514ms/step - loss: 0.2391 - val_loss: 0.1430\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 2s 501ms/step - loss: 0.2191 - val_loss: 0.1260\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 3s 509ms/step - loss: 0.2007 - val_loss: 0.1110\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 471ms/step - loss: 0.1838 - val_loss: 0.0979\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.1685 - val_loss: 0.0864\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.1546 - val_loss: 0.0765\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.1420 - val_loss: 0.0678\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.1307 - val_loss: 0.0603\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.1205 - val_loss: 0.0538\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.1112 - val_loss: 0.0481\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.1029 - val_loss: 0.0431\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.0954 - val_loss: 0.0388\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.0887 - val_loss: 0.0350\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.0826 - val_loss: 0.0317\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.0770 - val_loss: 0.0288\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.0720 - val_loss: 0.0263\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.0675 - val_loss: 0.0240\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 462ms/step - loss: 0.0633 - val_loss: 0.0220\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.0595 - val_loss: 0.0202\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.0560 - val_loss: 0.0186\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 406ms/step - loss: 0.0529 - val_loss: 0.0171\n"
     ]
    }
   ],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 50\n",
    "\n",
    "# this is our input placeholder\n",
    "input_gene = Input(shape=(60483,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_gene)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(60483, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_gene, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_gene, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (50-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "autoencoder.fit(train_all_x, train_all_x,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_all_x, test_all_x))\n",
    "\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_genes = encoder.predict(test_all_x)\n",
    "decoded_genes = decoder.predict(encoded_genes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135791d-929b-4d06-b6ee-448f3c3fb696",
   "metadata": {},
   "source": [
    "The model continues to stablize with more epochs, eventually getting to a loss of only 1.7% between the training and the test sets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a46742-5265-47e2-ad43-3e0f8594020f",
   "metadata": {},
   "source": [
    "*Question 2* \n",
    "\n",
    "In this problem, we vary the size of the \"bottleneck layer\" or the smallest dimension representation that the data get compressed to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae3b1ef5-84a3-4cb0-b83c-afaa3bb44781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning autoencoding with dimension size:  10\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 3s 580ms/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.6904 - val_loss: 0.6889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.6882 - val_loss: 0.6867\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 463ms/step - loss: 0.6859 - val_loss: 0.6844\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 408ms/step - loss: 0.6836 - val_loss: 0.6820\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.6812 - val_loss: 0.6795\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.6787 - val_loss: 0.6769\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 415ms/step - loss: 0.6761 - val_loss: 0.6741\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.6733 - val_loss: 0.6710\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.6703 - val_loss: 0.6675\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.6670 - val_loss: 0.6635\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 413ms/step - loss: 0.6633 - val_loss: 0.6589\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 0.6592 - val_loss: 0.6537\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 419ms/step - loss: 0.6546 - val_loss: 0.6477\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.6494 - val_loss: 0.6409\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.6436 - val_loss: 0.6331\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 411ms/step - loss: 0.6372 - val_loss: 0.6244\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 428ms/step - loss: 0.6302 - val_loss: 0.6146\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.6224 - val_loss: 0.6039\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.6139 - val_loss: 0.5922\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.6048 - val_loss: 0.5795\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.5949 - val_loss: 0.5659\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.5845 - val_loss: 0.5514\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.5734 - val_loss: 0.5361\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 2s 365ms/step - loss: 0.5617 - val_loss: 0.5201\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.5495 - val_loss: 0.5035\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.5368 - val_loss: 0.4864\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 2s 424ms/step - loss: 0.5237 - val_loss: 0.4689\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.5102 - val_loss: 0.4510\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.4964 - val_loss: 0.4330\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.4824 - val_loss: 0.4149\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.4681 - val_loss: 0.3967\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.4538 - val_loss: 0.3788\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.4394 - val_loss: 0.3610\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 454ms/step - loss: 0.4249 - val_loss: 0.3434\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.4106 - val_loss: 0.3262\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.3963 - val_loss: 0.3094\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3821 - val_loss: 0.2931\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3682 - val_loss: 0.2773\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3545 - val_loss: 0.2621\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3410 - val_loss: 0.2474\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3279 - val_loss: 0.2334\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3150 - val_loss: 0.2199\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.3026 - val_loss: 0.2072\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2905 - val_loss: 0.1950\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.2788 - val_loss: 0.1835\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 366ms/step - loss: 0.2675 - val_loss: 0.1726\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2566 - val_loss: 0.1623\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2461 - val_loss: 0.1526\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 366ms/step - loss: 0.2360 - val_loss: 0.1434\n",
      "Completed autoencoding with dimension size:  10\n",
      "Beginning autoencoding with dimension size:  20\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 3s 554ms/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.6904 - val_loss: 0.6889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.6882 - val_loss: 0.6867\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.6859 - val_loss: 0.6844\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 0.6836 - val_loss: 0.6819\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.6811 - val_loss: 0.6794\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 411ms/step - loss: 0.6786 - val_loss: 0.6766\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.6758 - val_loss: 0.6736\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 423ms/step - loss: 0.6728 - val_loss: 0.6701\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 396ms/step - loss: 0.6695 - val_loss: 0.6661\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.6657 - val_loss: 0.6614\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.6615 - val_loss: 0.6558\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.6566 - val_loss: 0.6493\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.6510 - val_loss: 0.6418\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 3s 547ms/step - loss: 0.6446 - val_loss: 0.6330\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 4s 707ms/step - loss: 0.6374 - val_loss: 0.6230\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.6293 - val_loss: 0.6117\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.6203 - val_loss: 0.5991\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.6103 - val_loss: 0.5851\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.5994 - val_loss: 0.5699\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.5877 - val_loss: 0.5534\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.5750 - val_loss: 0.5359\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 2s 422ms/step - loss: 0.5615 - val_loss: 0.5173\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 2s 415ms/step - loss: 0.5473 - val_loss: 0.4979\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.5324 - val_loss: 0.4777\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 422ms/step - loss: 0.5168 - val_loss: 0.4570\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 2s 411ms/step - loss: 0.5008 - val_loss: 0.4360\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.4844 - val_loss: 0.4147\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.4676 - val_loss: 0.3934\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4507 - val_loss: 0.3723\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.4336 - val_loss: 0.3514\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 2s 369ms/step - loss: 0.4166 - val_loss: 0.3309\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3996 - val_loss: 0.3110\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 0.3828 - val_loss: 0.2916\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 496ms/step - loss: 0.3662 - val_loss: 0.2730\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.3500 - val_loss: 0.2551\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.3341 - val_loss: 0.2381\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.3186 - val_loss: 0.2219\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.3036 - val_loss: 0.2066\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.2891 - val_loss: 0.1921\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 2s 459ms/step - loss: 0.2751 - val_loss: 0.1785\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.2615 - val_loss: 0.1657\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.2486 - val_loss: 0.1537\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.2361 - val_loss: 0.1425\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 2s 502ms/step - loss: 0.2242 - val_loss: 0.1320\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 3s 675ms/step - loss: 0.2128 - val_loss: 0.1223\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 410ms/step - loss: 0.2019 - val_loss: 0.1133\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.1916 - val_loss: 0.1050\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 415ms/step - loss: 0.1818 - val_loss: 0.0972\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.1725 - val_loss: 0.0901\n",
      "Completed autoencoding with dimension size:  20\n",
      "Beginning autoencoding with dimension size:  30\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 3s 564ms/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.6904 - val_loss: 0.6889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.6882 - val_loss: 0.6867\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.6859 - val_loss: 0.6843\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.6835 - val_loss: 0.6819\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 428ms/step - loss: 0.6810 - val_loss: 0.6792\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 410ms/step - loss: 0.6784 - val_loss: 0.6763\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.6755 - val_loss: 0.6730\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 421ms/step - loss: 0.6722 - val_loss: 0.6691\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 454ms/step - loss: 0.6686 - val_loss: 0.6645\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.6643 - val_loss: 0.6590\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.6595 - val_loss: 0.6525\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.6538 - val_loss: 0.6449\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.6473 - val_loss: 0.6360\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.6400 - val_loss: 0.6257\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 480ms/step - loss: 0.6316 - val_loss: 0.6139\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 485ms/step - loss: 0.6221 - val_loss: 0.6006\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 425ms/step - loss: 0.6116 - val_loss: 0.5857\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 2s 499ms/step - loss: 0.6000 - val_loss: 0.5693\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 2s 463ms/step - loss: 0.5872 - val_loss: 0.5514\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 2s 454ms/step - loss: 0.5733 - val_loss: 0.5320\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.5583 - val_loss: 0.5112\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.5423 - val_loss: 0.4893\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 2s 468ms/step - loss: 0.5253 - val_loss: 0.4664\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.5075 - val_loss: 0.4428\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 421ms/step - loss: 0.4890 - val_loss: 0.4187\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.4699 - val_loss: 0.3943\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.4504 - val_loss: 0.3700\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 2s 408ms/step - loss: 0.4306 - val_loss: 0.3459\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.4108 - val_loss: 0.3222\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.3910 - val_loss: 0.2993\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 2s 400ms/step - loss: 0.3714 - val_loss: 0.2771\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.3521 - val_loss: 0.2560\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.3333 - val_loss: 0.2359\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 413ms/step - loss: 0.3150 - val_loss: 0.2170\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.2974 - val_loss: 0.1993\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.2804 - val_loss: 0.1827\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.2643 - val_loss: 0.1674\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.2489 - val_loss: 0.1533\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.2343 - val_loss: 0.1403\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2205 - val_loss: 0.1284\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 2s 396ms/step - loss: 0.2075 - val_loss: 0.1176\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.1953 - val_loss: 0.1077\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.1839 - val_loss: 0.0986\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.1732 - val_loss: 0.0904\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.1632 - val_loss: 0.0830\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.1539 - val_loss: 0.0762\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.1451 - val_loss: 0.0700\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.1369 - val_loss: 0.0644\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.1293 - val_loss: 0.0594\n",
      "Completed autoencoding with dimension size:  30\n",
      "Beginning autoencoding with dimension size:  40\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 4s 669ms/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 0.6904 - val_loss: 0.6889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 4s 805ms/step - loss: 0.6882 - val_loss: 0.6867\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 3s 547ms/step - loss: 0.6859 - val_loss: 0.6843\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 3s 559ms/step - loss: 0.6834 - val_loss: 0.6817\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 0.6809 - val_loss: 0.6789\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.6780 - val_loss: 0.6756\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 3s 703ms/step - loss: 0.6747 - val_loss: 0.6716\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 422ms/step - loss: 0.6709 - val_loss: 0.6667\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 479ms/step - loss: 0.6664 - val_loss: 0.6607\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 415ms/step - loss: 0.6609 - val_loss: 0.6532\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 515ms/step - loss: 0.6543 - val_loss: 0.6440\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.6465 - val_loss: 0.6329\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 515ms/step - loss: 0.6372 - val_loss: 0.6196\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 3s 510ms/step - loss: 0.6264 - val_loss: 0.6041\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 3s 559ms/step - loss: 0.6140 - val_loss: 0.5862\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 479ms/step - loss: 0.5999 - val_loss: 0.5661\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.5841 - val_loss: 0.5438\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 2s 480ms/step - loss: 0.5667 - val_loss: 0.5195\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 3s 580ms/step - loss: 0.5478 - val_loss: 0.4934\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 3s 574ms/step - loss: 0.5274 - val_loss: 0.4660\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 2s 422ms/step - loss: 0.5060 - val_loss: 0.4376\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 0.4836 - val_loss: 0.4086\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.4604 - val_loss: 0.3795\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 2s 489ms/step - loss: 0.4369 - val_loss: 0.3506\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 473ms/step - loss: 0.4131 - val_loss: 0.3223\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.3894 - val_loss: 0.2950\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.3660 - val_loss: 0.2689\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 2s 486ms/step - loss: 0.3431 - val_loss: 0.2443\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 471ms/step - loss: 0.3209 - val_loss: 0.2212\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.2996 - val_loss: 0.1999\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.2792 - val_loss: 0.1802\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 483ms/step - loss: 0.2599 - val_loss: 0.1622\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.2417 - val_loss: 0.1459\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 468ms/step - loss: 0.2246 - val_loss: 0.1311\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.2087 - val_loss: 0.1178\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.1938 - val_loss: 0.1059\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.1801 - val_loss: 0.0953\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.1674 - val_loss: 0.0857\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.1557 - val_loss: 0.0773\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.1449 - val_loss: 0.0697\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.1350 - val_loss: 0.0630\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.1258 - val_loss: 0.0571\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.1175 - val_loss: 0.0518\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.1098 - val_loss: 0.0471\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.1028 - val_loss: 0.0429\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.0963 - val_loss: 0.0391\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.0903 - val_loss: 0.0357\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 0.0848 - val_loss: 0.0327\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.0798 - val_loss: 0.0300\n",
      "Completed autoencoding with dimension size:  40\n",
      "Beginning autoencoding with dimension size:  60\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 4s 682ms/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 477ms/step - loss: 0.6904 - val_loss: 0.6889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 485ms/step - loss: 0.6881 - val_loss: 0.6866\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 483ms/step - loss: 0.6858 - val_loss: 0.6841\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.6832 - val_loss: 0.6813\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 461ms/step - loss: 0.6803 - val_loss: 0.6779\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 463ms/step - loss: 0.6770 - val_loss: 0.6737\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.6729 - val_loss: 0.6683\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 3s 569ms/step - loss: 0.6677 - val_loss: 0.6612\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.6613 - val_loss: 0.6521\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 475ms/step - loss: 0.6533 - val_loss: 0.6404\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 482ms/step - loss: 0.6433 - val_loss: 0.6260\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 470ms/step - loss: 0.6313 - val_loss: 0.6083\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 487ms/step - loss: 0.6169 - val_loss: 0.5875\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 469ms/step - loss: 0.6001 - val_loss: 0.5633\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 491ms/step - loss: 0.5809 - val_loss: 0.5360\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 478ms/step - loss: 0.5594 - val_loss: 0.5060\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 3s 536ms/step - loss: 0.5358 - val_loss: 0.4736\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 3s 551ms/step - loss: 0.5102 - val_loss: 0.4396\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 3s 535ms/step - loss: 0.4831 - val_loss: 0.4046\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 2s 503ms/step - loss: 0.4549 - val_loss: 0.3694\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 2s 495ms/step - loss: 0.4261 - val_loss: 0.3347\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 3s 518ms/step - loss: 0.3970 - val_loss: 0.3010\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 2s 470ms/step - loss: 0.3683 - val_loss: 0.2690\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 2s 499ms/step - loss: 0.3401 - val_loss: 0.2390\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 487ms/step - loss: 0.3130 - val_loss: 0.2114\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.2871 - val_loss: 0.1863\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.2628 - val_loss: 0.1637\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.2402 - val_loss: 0.1436\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.2192 - val_loss: 0.1259\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.2000 - val_loss: 0.1103\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 2s 469ms/step - loss: 0.1825 - val_loss: 0.0968\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 487ms/step - loss: 0.1667 - val_loss: 0.0850\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 3s 505ms/step - loss: 0.1523 - val_loss: 0.0748\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 467ms/step - loss: 0.1394 - val_loss: 0.0660\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 462ms/step - loss: 0.1277 - val_loss: 0.0584\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 467ms/step - loss: 0.1173 - val_loss: 0.0518\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 2s 506ms/step - loss: 0.1079 - val_loss: 0.0461\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 2s 487ms/step - loss: 0.0994 - val_loss: 0.0411\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 476ms/step - loss: 0.0919 - val_loss: 0.0368\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 3s 544ms/step - loss: 0.0851 - val_loss: 0.0331\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 3s 523ms/step - loss: 0.0789 - val_loss: 0.0299\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 3s 521ms/step - loss: 0.0734 - val_loss: 0.0270\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 473ms/step - loss: 0.0684 - val_loss: 0.0245\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 3s 515ms/step - loss: 0.0638 - val_loss: 0.0223\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 2s 462ms/step - loss: 0.0597 - val_loss: 0.0203\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 471ms/step - loss: 0.0559 - val_loss: 0.0186\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 3s 506ms/step - loss: 0.0525 - val_loss: 0.0171\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 468ms/step - loss: 0.0494 - val_loss: 0.0157\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 0.0465 - val_loss: 0.0145\n",
      "Completed autoencoding with dimension size:  60\n",
      "Beginning autoencoding with dimension size:  80\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 5s 724ms/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 471ms/step - loss: 0.6904 - val_loss: 0.6889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 3s 499ms/step - loss: 0.6881 - val_loss: 0.6866\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 3s 536ms/step - loss: 0.6858 - val_loss: 0.6841\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 3s 517ms/step - loss: 0.6832 - val_loss: 0.6812\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 3s 524ms/step - loss: 0.6803 - val_loss: 0.6778\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 483ms/step - loss: 0.6769 - val_loss: 0.6735\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 3s 511ms/step - loss: 0.6726 - val_loss: 0.6677\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 510ms/step - loss: 0.6672 - val_loss: 0.6602\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 3s 516ms/step - loss: 0.6603 - val_loss: 0.6503\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 492ms/step - loss: 0.6516 - val_loss: 0.6377\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 3s 536ms/step - loss: 0.6408 - val_loss: 0.6219\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 3s 524ms/step - loss: 0.6277 - val_loss: 0.6026\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 494ms/step - loss: 0.6120 - val_loss: 0.5796\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 483ms/step - loss: 0.5934 - val_loss: 0.5530\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 480ms/step - loss: 0.5724 - val_loss: 0.5231\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 488ms/step - loss: 0.5486 - val_loss: 0.4903\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 3s 570ms/step - loss: 0.5226 - val_loss: 0.4551\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 3s 513ms/step - loss: 0.4947 - val_loss: 0.4184\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 3s 537ms/step - loss: 0.4651 - val_loss: 0.3811\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 3s 538ms/step - loss: 0.4346 - val_loss: 0.3439\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.4036 - val_loss: 0.3077\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 3s 590ms/step - loss: 0.3727 - val_loss: 0.2731\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 3s 535ms/step - loss: 0.3423 - val_loss: 0.2407\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 3s 536ms/step - loss: 0.3131 - val_loss: 0.2109\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 483ms/step - loss: 0.2852 - val_loss: 0.1839\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 3s 537ms/step - loss: 0.2590 - val_loss: 0.1598\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 3s 498ms/step - loss: 0.2347 - val_loss: 0.1386\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 3s 507ms/step - loss: 0.2123 - val_loss: 0.1200\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.1920 - val_loss: 0.1039\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 3s 526ms/step - loss: 0.1736 - val_loss: 0.0901\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.1571 - val_loss: 0.0782\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 485ms/step - loss: 0.1424 - val_loss: 0.0681\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 3s 529ms/step - loss: 0.1293 - val_loss: 0.0595\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 493ms/step - loss: 0.1176 - val_loss: 0.0521\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 494ms/step - loss: 0.1072 - val_loss: 0.0458\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.0980 - val_loss: 0.0405\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 3s 518ms/step - loss: 0.0898 - val_loss: 0.0359\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 3s 503ms/step - loss: 0.0825 - val_loss: 0.0319\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 484ms/step - loss: 0.0760 - val_loss: 0.0285\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 3s 548ms/step - loss: 0.0702 - val_loss: 0.0256\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 3s 541ms/step - loss: 0.0651 - val_loss: 0.0231\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 2s 496ms/step - loss: 0.0604 - val_loss: 0.0209\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 469ms/step - loss: 0.0563 - val_loss: 0.0189\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.0525 - val_loss: 0.0172\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 2s 463ms/step - loss: 0.0490 - val_loss: 0.0157\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.0460 - val_loss: 0.0144\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 2s 490ms/step - loss: 0.0432 - val_loss: 0.0132\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.0406 - val_loss: 0.0122\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 412ms/step - loss: 0.0382 - val_loss: 0.0112\n",
      "Completed autoencoding with dimension size:  80\n"
     ]
    }
   ],
   "source": [
    "#Different \"bottleneck\" (encoding dimension) layer sizes \n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_values = [10, 20, 30, 40, 60, 80]\n",
    "\n",
    "\n",
    "for i in range(6): \n",
    "    \n",
    "\n",
    "    # this is the size of our encoded representations\n",
    "    encoding_dim = encoding_values[i]\n",
    "    \n",
    "    print(\"Beginning autoencoding with dimension size: \", encoding_dim)\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_gene = Input(shape=(60483,))\n",
    "\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_gene)\n",
    "\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(60483, activation='sigmoid')(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_gene, decoded)\n",
    "\n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = Model(input_gene, encoded)\n",
    "\n",
    "    # create a placeholder for an encoded (50-dimensional) input\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "    # retrieve the last layer of the autoencoder model\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "    # create the decoder model\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "    autoencoder.fit(train_all_x, train_all_x,\n",
    "                    epochs=50,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_all_x, test_all_x))\n",
    "\n",
    "\n",
    "    # encode and decode some digits\n",
    "    # note that we take them from the *test* set\n",
    "    encoded_genes = encoder.predict(test_all_x)\n",
    "    decoded_genes = decoder.predict(encoded_genes)\n",
    "    \n",
    "    print(\"Completed autoencoding with dimension size: \", encoding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d234d-fd9d-4007-8133-12d7fea83a3f",
   "metadata": {},
   "source": [
    "Ten dimensions is too small to fully capture the data and when we decode it, we still have meaningful loss between the training and test sets even at the last epoch (14%). The most stable and best performing model is one with 80 dimensions and only has a loss of 1.1% at the final epoch. Since these data have so many features, we perhaps need more dimensions in order to best capture the variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8415f7-edb0-4947-be36-f0ec106be173",
   "metadata": {},
   "source": [
    "*Question 3*\n",
    "\n",
    "We've seen what happens when we have a single layer. I actually don't think a single layer model performs that poorly (1.1% loss rate in the best performing one). However, it takes many epochs to finally stabilize which is tedious and perhaps requires a larger bottleneck layer size to optimize. Now let's explore what happens with 4 & 20 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5fe4f2ab-f06f-4d40-b4e8-723fb01d3f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.6925 - val_loss: 0.6911\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 3s 568ms/step - loss: 0.6904 - val_loss: 0.6888\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 3s 545ms/step - loss: 0.6879 - val_loss: 0.6859\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 3s 576ms/step - loss: 0.6845 - val_loss: 0.6809\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 3s 549ms/step - loss: 0.6784 - val_loss: 0.6700\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 3s 570ms/step - loss: 0.6651 - val_loss: 0.6441\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 3s 572ms/step - loss: 0.6351 - val_loss: 0.5850\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 3s 565ms/step - loss: 0.5717 - val_loss: 0.4680\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 3s 588ms/step - loss: 0.4548 - val_loss: 0.2885\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 3s 566ms/step - loss: 0.2858 - val_loss: 0.1116\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 3s 560ms/step - loss: 0.1225 - val_loss: 0.0240\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 3s 567ms/step - loss: 0.0347 - val_loss: 0.0039\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 3s 551ms/step - loss: 0.0083 - val_loss: 9.5048e-04\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 3s 576ms/step - loss: 0.0024 - val_loss: 5.3564e-04\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 3s 585ms/step - loss: 0.0010 - val_loss: 4.7316e-04\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 3s 576ms/step - loss: 5.7701e-04 - val_loss: 4.6576e-04\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 3s 574ms/step - loss: 4.2148e-04 - val_loss: 4.6524e-04\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 3s 581ms/step - loss: 3.5009e-04 - val_loss: 4.6290e-04\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 3s 558ms/step - loss: 3.1270e-04 - val_loss: 4.5813e-04\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 3s 562ms/step - loss: 2.9127e-04 - val_loss: 4.5223e-04\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 3s 550ms/step - loss: 2.7811e-04 - val_loss: 4.4652e-04\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 3s 560ms/step - loss: 2.6949e-04 - val_loss: 4.4237e-04\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 3s 608ms/step - loss: 2.6337e-04 - val_loss: 4.3993e-04\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 3s 543ms/step - loss: 2.5871e-04 - val_loss: 4.3874e-04\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 3s 553ms/step - loss: 2.5501e-04 - val_loss: 4.3791e-04\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 3s 563ms/step - loss: 2.5192e-04 - val_loss: 4.3735e-04\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 3s 572ms/step - loss: 2.4914e-04 - val_loss: 4.3675e-04\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 3s 561ms/step - loss: 2.4658e-04 - val_loss: 4.3609e-04\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 3s 568ms/step - loss: 2.4414e-04 - val_loss: 4.3523e-04\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 3s 591ms/step - loss: 2.4182e-04 - val_loss: 4.3431e-04\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 3s 585ms/step - loss: 2.3961e-04 - val_loss: 4.3383e-04\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 3s 575ms/step - loss: 2.3747e-04 - val_loss: 4.3338e-04\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 3s 554ms/step - loss: 2.3535e-04 - val_loss: 4.3316e-04\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 3s 570ms/step - loss: 2.3329e-04 - val_loss: 4.3263e-04\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 3s 589ms/step - loss: 2.3128e-04 - val_loss: 4.3244e-04\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 3s 571ms/step - loss: 2.2930e-04 - val_loss: 4.3218e-04\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 3s 556ms/step - loss: 2.2734e-04 - val_loss: 4.3189e-04\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 3s 625ms/step - loss: 2.2543e-04 - val_loss: 4.3134e-04\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 3s 574ms/step - loss: 2.2356e-04 - val_loss: 4.3095e-04\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 3s 558ms/step - loss: 2.2174e-04 - val_loss: 4.3100e-04\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 3s 559ms/step - loss: 2.1992e-04 - val_loss: 4.3085e-04\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 3s 561ms/step - loss: 2.1817e-04 - val_loss: 4.3064e-04\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 3s 575ms/step - loss: 2.1645e-04 - val_loss: 4.3032e-04\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 3s 565ms/step - loss: 2.1476e-04 - val_loss: 4.3025e-04\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 3s 543ms/step - loss: 2.1311e-04 - val_loss: 4.3037e-04\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 3s 539ms/step - loss: 2.1150e-04 - val_loss: 4.3022e-04\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 3s 526ms/step - loss: 2.0992e-04 - val_loss: 4.3011e-04\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 3s 554ms/step - loss: 2.0838e-04 - val_loss: 4.3018e-04\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 3s 529ms/step - loss: 2.0687e-04 - val_loss: 4.3006e-04\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 3s 520ms/step - loss: 2.0541e-04 - val_loss: 4.2983e-04\n"
     ]
    }
   ],
   "source": [
    "#Autoencoding\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 80\n",
    "\n",
    "# this is our input placeholder\n",
    "input_gene = Input(shape=(60483,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded_1 = layers.Dense(128, activation='relu', name= \"encoded_1\")(input_gene)\n",
    "encoded_2 = layers.Dense(encoding_dim, activation='relu', name = \"encoded_2\")(encoded_1)\n",
    "#encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded_1 = layers.Dense(128, activation='relu')(encoded_2)\n",
    "decoded_2 = layers.Dense(60483, activation='sigmoid', name = \"decoded_2\")(decoded_1)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_gene, decoded_2)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "autoencoder.fit(train_all_x, train_all_x,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_all_x, test_all_x))\n",
    "\n",
    "\n",
    "\n",
    "# Making the encoder model\n",
    "new_encoded_1 = autoencoder.layers[1]\n",
    "new_encoded_2 = autoencoder.layers[2]\n",
    "\n",
    "encoder = Model(input_gene, new_encoded_2.output)\n",
    "#encoder.summary()\n",
    "\n",
    "\n",
    "#The decoder model\n",
    "encoded_input = Input(shape=(encoding_dim,), name='encoded_input')\n",
    "new_decoded_1 = autoencoder.layers[-2](encoded_input)\n",
    "new_decoded_2 = autoencoder.layers[-1](new_decoded_1)\n",
    "\n",
    "decoder = Model(encoded_input, new_decoded_2)\n",
    "#decoder.summary()\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_genes = encoder.predict(test_all_x)\n",
    "decoded_genes = decoder.predict(encoded_genes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b019d81-2112-4830-9f21-6e2d54f3e87e",
   "metadata": {},
   "source": [
    "A 4-layer autoencoder removes pratically all variation between the test and training set accuracy by just the 13th epoch. This allows for highly accurate results with fewer iterations needed because more layers handle this work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2691c9-743b-481b-8d85-361e04df321f",
   "metadata": {},
   "source": [
    "However, if we include 20 layers, this is too many and reduces the model's usefulness. It takes forever to run and doesn't result in an meaningful improvement in model accuracy. This demonstrates to me that deciding the optimal number of layers is yet another step in the tuning process to identify the best autoencoder. I was able to run this earlier and the results weren't any different from 4 layers. Now it won't even complete its run because it takes too much memory. I'd want to experiment in the future with parallelizing this or running it on AWS. But since it doesn't add much, we're not really missing out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2ecef-b3d7-4909-ace7-8746ae2fa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoding\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 80\n",
    "\n",
    "# this is our input placeholder\n",
    "input_gene = Input(shape=(60483,))\n",
    "\n",
    "#20 layers \n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded_1 = layers.Dense(5000, activation='relu', name= \"encoded_1\")(input_gene)\n",
    "encoded_2 = layers.Dense(4000, activation='relu', name = \"encoded_2\")(encoded_1)\n",
    "encoded_3 = layers.Dense(3000, activation='relu', name = \"encoded_3\")(encoded_2)\n",
    "encoded_4 = layers.Dense(2000, activation='relu', name = \"encoded_4\")(encoded_3)\n",
    "encoded_5 = layers.Dense(1000, activation='relu', name = \"encoded_5\")(encoded_4)\n",
    "encoded_6 = layers.Dense(500, activation='relu', name = \"encoded_6\")(encoded_5)\n",
    "encoded_7 = layers.Dense(250, activation='relu', name = \"encoded_7\")(encoded_6)\n",
    "encoded_8 = layers.Dense(200, activation='relu', name = \"encoded_8\")(encoded_7)\n",
    "encoded_9 = layers.Dense(150, activation='relu', name = \"encoded_9\")(encoded_8)\n",
    "\n",
    "\n",
    "encoded_final = layers.Dense(encoding_dim, activation='relu', name = \"encoded_final\")(encoded_9)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded_1 = layers.Dense(150, activation='relu', name = \"decoded_1\")(encoded_final)\n",
    "decoded_2 = layers.Dense(200, activation='relu', name = \"decoded_2\")(decoded_1)\n",
    "decoded_3 = layers.Dense(250, activation='relu', name = \"decoded_3\")(decoded_2)\n",
    "decoded_4 = layers.Dense(500, activation='relu', name = \"decoded_4\")(decoded_3)\n",
    "decoded_5 = layers.Dense(1000, activation='relu', name = \"decoded_5\")(decoded_4)\n",
    "decoded_6 = layers.Dense(2000, activation='relu', name = \"decoded_6\")(decoded_5)\n",
    "decoded_7 = layers.Dense(3000, activation='relu', name = \"decoded_7\")(decoded_6)\n",
    "decoded_8 = layers.Dense(4000, activation='relu', name = \"decoded_8\")(decoded_7)\n",
    "decoded_9 = layers.Dense(5000, activation='relu', name = \"decoded_9\")(decoded_8)\n",
    "\n",
    "\n",
    "decoded_final = layers.Dense(60483, activation='sigmoid', name = \"decoded_final\")(decoded_9)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_gene, decoded_final)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "autoencoder.fit(train_all_x, train_all_x,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_all_x, test_all_x))\n",
    "\n",
    "\n",
    "\n",
    "# Making the encoder model\n",
    "new_encoded_final = autoencoder.layers[10]\n",
    "\n",
    "encoder = Model(input_gene, new_encoded_final.output)\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "\n",
    "#The decoder model\n",
    "encoded_input = Input(shape=(encoding_dim,), name='encoded_input')\n",
    "\n",
    "new_decoded_1 = autoencoder.layers[-10](encoded_input)\n",
    "new_decoded_2 = autoencoder.layers[-9](new_decoded_1)\n",
    "new_decoded_3 = autoencoder.layers[-8](new_decoded_2)\n",
    "new_decoded_4 = autoencoder.layers[-7](new_decoded_3)\n",
    "new_decoded_5 = autoencoder.layers[-6](new_decoded_4)\n",
    "new_decoded_6 = autoencoder.layers[-5](new_decoded_5)\n",
    "new_decoded_7 = autoencoder.layers[-4](new_decoded_6)\n",
    "new_decoded_8 = autoencoder.layers[-3](new_decoded_7)\n",
    "new_decoded_9 = autoencoder.layers[-2](new_decoded_8)\n",
    "\n",
    "new_decoded_final = autoencoder.layers[-1](new_decoded_9)\n",
    "\n",
    "\n",
    "decoder = Model(encoded_input, new_decoded_final)\n",
    "decoder.summary()\n",
    "\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_genes = encoder.predict(test_all_x)\n",
    "decoded_genes = decoder.predict(encoded_genes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146b5f5-ac19-44f8-8195-ca48512e38ef",
   "metadata": {},
   "source": [
    "*Question 4* \n",
    "\n",
    "Two possible uses of an autoencoder for gene expression data include using the bottleneck layer as a way to select features and we can use the decoder to generate synthetic data. An autoencoder first compresses data down to the number of dimensions as the bottleneck layer and we can use this representation as we would any other dimension reduction method. Gene expression data is particularly complicated and numerous, if we can identify a simplified representation, then we can better understand, analyze, and interpret our results. For example, we could cluster these data to find certain meaningful similarities between genes in tumors. We could also generate synthetic data from decoding this representation which perhaps could be used to fix data imbalances. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
